package com.itenlee.search.analysis.core;

import com.hankcs.hanlp.HanLP;
import com.itenlee.search.analysis.algorithm.DoubleArrayTriePro;
import com.itenlee.search.analysis.algorithm.TokenNode;
import com.itenlee.search.analysis.help.DateUtil;
import com.itenlee.search.analysis.help.ESPluginLoggerFactory;
import com.itenlee.search.analysis.help.HttpClientUtil;
import com.itenlee.search.analysis.help.TextUtility;
import com.itenlee.search.analysis.lucence.Configuration;
import com.vdurmont.emoji.CustomEmojiParser;
import okhttp3.Response;
import org.apache.logging.log4j.Logger;

import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.security.AccessController;
import java.security.PrivilegedAction;
import java.security.PrivilegedActionException;
import java.util.*;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * @author tenlee
 * @date 2020/7/10
 */
public class Dictionary {
    private static final Logger logger = ESPluginLoggerFactory.getLogger(Dictionary.class.getName());

    private static final Pattern URL_NUM_ALPHA_CH_NUM_REG = Pattern.compile(
        "(?<url>(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|])|(?<num>[0-9]+(\\.[0-9]+)*)|(?<alpha>[a-zA-Z]+)");
    private static final Pattern CH_NUM_REG = Pattern.compile("[ä¸€ä¸¤äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶][ä¸€ä¸¤äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶ç™¾åƒä¸‡å…†äº¿]*");
    private static final String PUNC = TextUtility.sbc2dbcCase(" ã€‚[],?!+-*/'\"â€¦<>=ã€Šã€‹`~ï¼@#ï¿¥%â€¦â€¦&();:{}\\.|^_ã€ã€ã€Œ");

    private static volatile Dictionary instance;
    private static ScheduledExecutorService pool = Executors.newScheduledThreadPool(1);
    private static HttpClientUtil httpClient = HttpClientUtil.getInstance();

    private Configuration configuration;
    private HashSet<String> metaWords = new HashSet<>();
    private long total;
    private double logTotal;
    private DoubleArrayTriePro doubleArrayTrie = new DoubleArrayTriePro();

    public static Dictionary getInstance() {
        if (instance == null) {
            throw new IllegalStateException("hao dict has not been initialized yet, please call initial method first.");
        }
        return instance;
    }

    synchronized public static void initial(Configuration cfg) {
        if (instance == null) {
            synchronized (Dictionary.class) {
                if (instance == null) {
                    try {
                        instance = new Dictionary(cfg);

                        instance.loadDict();
                        if ((cfg.getRemoteFreqDict() != null) && (cfg.getRemoteFreqDict().trim().length() > 0)) {
                            if (cfg.getSyncDicPeriodTime() == null || cfg.getSyncDicPeriodTime() < 30) {
                                logger.warn("syncDicPeriodTime illegal: must >= 30");
                            } else {
                                // å»ºç«‹ç›‘æ§çº¿ç¨‹
                                logger.info("start monitor when {}", cfg.getSyncDicTime());
                                long initialDelay =
                                    TextUtility.isEmpty(cfg.getSyncDicTime()) ? cfg.getSyncDicPeriodTime() :
                                        DateUtil.calcTimeGap(cfg.getSyncDicTime());
                                pool.scheduleAtFixedRate(new Monitor(cfg.getRemoteFreqDict(), cfg), initialDelay,
                                    cfg.getSyncDicPeriodTime(), TimeUnit.SECONDS);
                            }
                        }
                        logger.info("hao dic init ok");
                    } catch (Exception e) {
                        throw new RuntimeException(e);
                    }
                }
            }
        }
    }

    private Dictionary() throws IllegalAccessException {
        throw new IllegalAccessException();
    }

    private Dictionary(Configuration configuration) throws Exception {
        this.configuration = configuration;
    }

    /**
     * è¯»å–META_WORDS_FILEï¼Œé‡Œè¾¹çš„è¯ä¸å†åšæ›´ç»†é¢—ç²’åº¦çš„åˆ†å‰²
     */
    private void loadDict() throws Exception {
        this.total = 0;
        TreeMap<String, Double> wordFreqMap = new TreeMap<>();
        try (InputStream is = new FileInputStream(configuration.getBaseDictionaryFile());
            BufferedReader br = new BufferedReader(new InputStreamReader(is, "UTF-8"))) {
            String line;
            while ((line = br.readLine()) != null) {
                if (line.length() == 0) {
                    continue;
                }
                String[] wordFreq = line.split(",");
                double freq = Double.parseDouble(wordFreq[1]);
                wordFreqMap.put(wordFreq[0], freq);
                this.total += freq;
            }
        } catch (IOException e) {
            logger.error("base dictionary load fail:{}", e.getMessage());
        }

        this.loadCustomerDictionary(wordFreqMap);
        this.loadRemoteDictionary(wordFreqMap);
        this.logTotal = Math.log(total);
        logger.info("wordFreqMap size: {}, hao dic start to buildTrie ... ", wordFreqMap.size());
        this.buildTrie(wordFreqMap);
    }

    /**
     * åŠ è½½è¿œç¨‹è¯åº“
     *
     * @param wordFreqMap
     */
    private void loadRemoteDictionary(Map<String, Double> wordFreqMap) {
        String location = this.configuration.getRemoteFreqDict();
        if (location == null || location.isEmpty()) {
            return;
        }
        Response response = null;
        try {
            response = httpClient.get(location);
        } catch (PrivilegedActionException e) {
            logger.warn("loadRemoteDictionary error", e);
            return;
        }
        //è¿”å›200 æ‰åšæ“ä½œ
        if (response.body() == null || response.code() != 200) {
            logger.warn("loadRemoteDictionary remote status is not 200, actual {}", response.code());
            return;
        }
        // è¿œç¨‹è¯åº“æœ‰æ›´æ–°,éœ€è¦é‡æ–°åŠ è½½è¯å…¸ï¼Œå¹¶ä¿®æ”¹last_modified,eTags
        Monitor.lastModified = response.header("Last-Modified");
        Monitor.eTags = response.header("ETag");
        try (BufferedReader in = new BufferedReader(response.body().charStream())) {
            String line;
            while ((line = in.readLine()) != null) {
                processDicLine(wordFreqMap, line);
            }
        } catch (Exception e) {
            logger.warn("loadRemoteDictionary error", e);
        } finally {
            response.close();
        }
    }

    /**
     * è¯»å–åŠ è½½å¤–éƒ¨è‡ªå®šä¹‰è¯å…¸
     *
     * @param wordFreqMap
     * @return
     * @throws IOException
     */
    private void loadCustomerDictionary(Map<String, Double> wordFreqMap) throws IOException {
        if (configuration.getCustomerDictionaryFiles() == null) {
            return;
        }
        for (String customerDictionaryFile : configuration.getCustomerDictionaryFiles()) {
            try (InputStream is = new FileInputStream(customerDictionaryFile);
                BufferedReader br = new BufferedReader(new InputStreamReader(is, "UTF-8"), 10240)) {
                String line;
                while ((line = br.readLine()) != null) {
                    if (line.length() == 0) {
                        continue;
                    }
                    processDicLine(wordFreqMap, line);
                }
            } catch (IOException e) {
                logger.error("custom dictionary load fail:{}", e.getMessage(), e);
            }
        }
        return;
    }

    private void processDicLine(Map<String, Double> wordFreqMap, String line) {
        String[] wordFreq = line.split(",");
        String cleanWord = clean(wordFreq[0]); // æŠŠè‡ªå®šä¹‰çš„è¯å½’ä¸€åŒ–å¤„ç†
        if (wordFreq.length == 3 && "1".equals(wordFreq[2])) {
            // æ˜¯å…ƒè¯
            this.metaWords.add(cleanWord);
        }
        //å…ƒè¯ä¹Ÿè¦åŠ å…¥è¯å…¸
        if (wordFreq.length == 1) {
            wordFreqMap.put(cleanWord, 100000.0);
        } else {
            Double freq = Double.parseDouble(wordFreq[1]);
            this.total += freq;
            wordFreqMap.put(cleanWord, freq);
        }
    }

    /**
     * ä»è¯å…¸æ„å»ºåŒæ•°ç»„æ ‘
     */
    private void buildTrie(TreeMap<String, Double> wordFreqMap) {
        wordFreqMap.forEach((k, v) -> {
            //ä¿è¯äº†å•å­—çš„è¯é¢‘å¤§æ¦‚ç‡æ¯”è¯ä½
            if (k.length() > 1 && v <= 100) {
                v = 100 * v;
            } else if (k.length() == 1 && v >= 10000) {
                v = v / 1000;
            }
            wordFreqMap.put(k, Math.abs(this.logTotal - Math.log(v)));
        });

        this.doubleArrayTrie.build(wordFreqMap);
    }

    /**
     * å°†æ¯ä¸€ä¸ªå­—è½¬åŒ–ä¸ºç»“ç‚¹ï¼Œå‚¨å­˜å„ç§åˆ†è¯è¿‡ç¨‹ä¸­éœ€è¦çš„ä¸´æ—¶æ•°æ®
     *
     * @param sentence
     * @return
     */
    public static ArrayList<TokenNode> buildNodes(String sentence) {
        String cleanedSen = clean(sentence);
        Matcher matcher = URL_NUM_ALPHA_CH_NUM_REG.matcher(cleanedSen);
        // éä¸­æ–‡å­—ç¬¦çš„åœ¨åŸå­—ç¬¦ä¸²çš„åç§»é‡ï¼Œkey=start offset, value=end offset
        HashMap<Integer, Integer> alphaSpanMap = new HashMap<>();
        HashMap<Integer, String> alphaTagMap = new HashMap<>();
        while (matcher.find()) {
            alphaSpanMap.put(matcher.start(), matcher.end());
            if (matcher.start(TokenNode.URL_TAG) != -1) {
                alphaTagMap.put(matcher.start(), TokenNode.URL_TAG);
            } else if (matcher.start(TokenNode.NUM_TAG) != -1) {
                alphaTagMap.put(matcher.start(), TokenNode.NUM_TAG);
            } else if (matcher.start(TokenNode.ALPHA_TAG) != -1) {
                alphaTagMap.put(matcher.start(), TokenNode.ALPHA_TAG);
            }
        }
        // å¤„ç†emojiè¡¨æƒ…ï¼Œemojiè¡¨æƒ…çš„é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„ï¼Œæ¯”å¦‚ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦=8ï¼ŒğŸ˜‚=2
        List<CustomEmojiParser.CustomEmoji> emojis = CustomEmojiParser.getUnicodeCandidates(sentence);
        int start = 0, wordNum = 0, emojiIndex = 0;
        int senLength = sentence.length();
        ArrayList<TokenNode> nodes = new ArrayList<>();
        while (start < senLength) {
            TokenNode node;
            Integer alphaSpanEnd = alphaSpanMap.get(start);
            if (alphaSpanEnd != null) {
                // æ˜¯ç‰¹æ®Šç¬¦å·
                node = new TokenNode(sentence.substring(start, alphaSpanEnd), wordNum, alphaTagMap.get(start));
                node.setCleanedText(cleanedSen.substring(start, alphaSpanEnd));
                node.setStartOffset(start);
                start = alphaSpanEnd;
            } else {
                String subCleanStr;
                String tag = null;
                if (emojis.size() > emojiIndex && emojis.get(emojiIndex).getStartOffset() == start) {
                    // æ˜¯emoji
                    subCleanStr = emojis.get(emojiIndex).getEmojiStr();
                    // emojiå½“åšæ ‡ç‚¹ç¬¦å·å¤„ç†
                    tag = TokenNode.PUNC_TAG;
                    emojiIndex++;
                } else {
                    // ç‰¹æ®Šçš„æ±‰å­—é•¿åº¦ä¸ä¸€å®šæ˜¯1ï¼Œæ¯”å¦‚ ğ¡ƒ=2
                    subCleanStr = new String(Character.toChars(cleanedSen.codePointAt(start)));
                    if (PUNC.contains(subCleanStr)) {
                        // æ ‡ç‚¹ç¬¦å·
                        tag = TokenNode.PUNC_TAG;
                    }
                }
                node = new TokenNode(sentence.substring(start, start + subCleanStr.length()), wordNum, tag);
                node.setCleanedText(subCleanStr);
                node.setStartOffset(start);
                start = start + subCleanStr.length();
            }
            nodes.add(node);

            wordNum++;
        }
        return nodes;
    }

    private static String clean(String sentence) {
        sentence = sentence.replaceAll(TextUtility.WHITESPACE_CHARCLASS, " ");
        sentence = TextUtility.sbc2dbcCase(sentence);
        String sentenceFinal = sentence;
        String simpleCN = AccessController.doPrivileged((PrivilegedAction<String>)() -> {
            return HanLP.convertToSimplifiedChinese(sentenceFinal);
        });
        // å¦‚æœopenccè¯†åˆ«æˆåŠŸ,é•¿åº¦åº”è¯¥æ˜¯ç›¸ç­‰çš„
        //openccä¼šæŠŠä¸è®¤è¯†çš„ç¹ä½“å­—è½¬æ¢æˆä¸‹é¢ğ ¡ å­—ç¬¦,è€Œè¿™ä¸ªå­—ç¬¦çš„é•¿åº¦æ˜¯2
        if (simpleCN.length() == sentence.length()) {
            sentence = simpleCN;
        }
        return TextUtility.toLowerCase(sentence);
    }

    public void reLoadMainDict() throws Exception {
        logger.info("start to reload hao dict.");
        // æ–°å¼€ä¸€ä¸ªå®ä¾‹åŠ è½½è¯å…¸ï¼Œå‡å°‘åŠ è½½è¿‡ç¨‹å¯¹å½“å‰è¯å…¸ä½¿ç”¨çš„å½±å“
        Dictionary tmpDict = new Dictionary(configuration);
        tmpDict.loadDict();
        instance.total = tmpDict.total;
        instance.logTotal = tmpDict.logTotal;
        instance.doubleArrayTrie = tmpDict.doubleArrayTrie;
        instance.metaWords = tmpDict.metaWords;
        logger.info("reload hao dict finished.");
    }

    public HashSet<String> getMetaWords() {
        return metaWords;
    }

    public double getLogTotal() {
        return logTotal;
    }

    public DoubleArrayTriePro getDoubleArrayTrie() {
        return doubleArrayTrie;
    }
}
