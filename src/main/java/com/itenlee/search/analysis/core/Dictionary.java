package com.itenlee.search.analysis.core;

import com.fasterxml.jackson.core.type.TypeReference;
import com.hankcs.hanlp.HanLP;
import com.itenlee.search.analysis.algorithm.AhoCorasickDoubleArrayTrie;
import com.itenlee.search.analysis.algorithm.TokenNode;
import com.itenlee.search.analysis.help.DateUtil;
import com.itenlee.search.analysis.help.ESPluginLoggerFactory;
import com.itenlee.search.analysis.help.JSONUtil;
import com.itenlee.search.analysis.help.TextUtility;
import com.itenlee.search.analysis.lucence.Configuration;
import com.vdurmont.emoji.CustomEmojiParser;
import org.apache.logging.log4j.Logger;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.security.AccessController;
import java.security.PrivilegedAction;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.TreeMap;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * @author tenlee
 * @date 2020/7/10
 */
public class Dictionary {
    private static final Logger logger = ESPluginLoggerFactory.getLogger(Dictionary.class.getName());

    private static final Pattern URL_NUM_ALPHA_CH_NUM_REG = Pattern.compile("(?<url>(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|])|(?<num>[0-9]+(\\.[0-9]+)*)|(?<alpha>[a-zA-Z]+)");
    private static final Pattern CH_NUM_REG = Pattern.compile("[ä¸€ä¸¤äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶][ä¸€ä¸¤äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶ç™¾åƒä¸‡å…†äº¿]*");
    private static final String PUNC = TextUtility.sbc2dbcCase(" ã€‚[],?!+-*/'\"â€¦<>=ã€Šã€‹`~ï¼@#ï¿¥%â€¦â€¦&();:{}\\.|^_ã€ã€ã€Œ");

    private static volatile Dictionary instance;
    private static ScheduledExecutorService pool = Executors.newScheduledThreadPool(1);

    private Configuration configuration;
    private HashSet<String> metaWords = new HashSet<>();
    private long total;
    private double logTotal;
    private AhoCorasickDoubleArrayTrie doubleArrayTrie = new AhoCorasickDoubleArrayTrie();

    public static Dictionary getInstance() {
        if (instance == null) {
            throw new IllegalStateException("dict has not been initialized yet, please call initial method first.");
        }
        return instance;
    }

    synchronized public static void initial(Configuration cfg) {
        if (instance == null) {
            synchronized (Dictionary.class) {
                if (instance == null) {
                    try {
                        instance = new Dictionary(cfg);
                        instance.loadDict();

                        if (cfg.getRemoteFreqDict() != null) {
                            // å»ºç«‹ç›‘æ§çº¿ç¨‹
                            logger.info("start monitor when {}", cfg.getSyncDicTime());
                            // 600 ç§’æ˜¯åˆå§‹å»¶è¿Ÿå¯ä»¥ä¿®æ”¹çš„ 600æ˜¯é—´éš”æ—¶é—´ å•ä½ç§’
                            pool.scheduleAtFixedRate(new Monitor(cfg.getRemoteFreqDict(), cfg), DateUtil.calcTimeGap(cfg.getSyncDicTime()), cfg.getSyncDicPeriodTime(), TimeUnit.SECONDS);
                        }
                        logger.info("dic init ok");
                    } catch (Exception e) {
                        throw new RuntimeException(e);
                    }
                }
            }
        }
    }

    private Dictionary() throws IllegalAccessException {
        throw new IllegalAccessException();
    }

    private Dictionary(Configuration configuration) throws Exception {
        this.configuration = configuration;
    }

    /**
     * è¯»å–META_WORDS_FILEï¼Œé‡Œè¾¹çš„è¯ä¸å†åšæ›´ç»†é¢—ç²’åº¦çš„åˆ†å‰²
     */
    private void loadDict() throws Exception {
        TreeMap<String, Long> baseDictionary = new TreeMap<>();
        try {
            baseDictionary = JSONUtil.parseJSON(new File(configuration.getBaseDictionaryFile()), new TypeReference<TreeMap<String, Long>>() {
            });
        } catch (Exception e) {
            logger.error("base dictionary load fail:{}", e.getMessage());
        }
        TreeMap<String, Long> extFreq = this.loadCustomerDictionary();
        this.total = baseDictionary.values().stream().reduce(0L, Long::sum);
        this.total = baseDictionary.values().stream().reduce(this.total, Long::sum);
        this.logTotal = Math.log(total);
        this.buildTrie(baseDictionary, extFreq);
    }

    /**
     * è¯»å–åŠ è½½å¤–éƒ¨è‡ªå®šä¹‰è¯å…¸
     *
     * @return
     * @throws IOException
     */
    private TreeMap<String, Long> loadCustomerDictionary() throws IOException {
        TreeMap<String, Long> extFreq = new TreeMap<>();
        try (BufferedReader br = new BufferedReader(new FileReader(configuration.getCustomerDictionaryFile()))) {
            String line;
            while ((line = br.readLine()) != null) {
                if (line.length() == 0) {
                    continue;
                }
                String[] wordFreq = line.split(",");
                if (wordFreq.length == 3 && "1".equals(wordFreq[2])) {
                    // æ˜¯å…ƒè¯
                    this.metaWords.add(wordFreq[0]);
                }
                if (wordFreq.length == 1) {
                    extFreq.put(wordFreq[0], 100000L);
                } else {
                    extFreq.put(wordFreq[0], Long.parseLong(wordFreq[1]));
                }
            }

        } catch (IOException e) {
            logger.error("custom dictionary load fail:{}", e.getMessage(), e);
        }
        return extFreq;
    }

    /**
     * ä»è¯å…¸æ„å»ºå­—å…¸æ ‘
     */
    private void buildTrie(TreeMap<String, Long> freq, TreeMap<String, Long> extFreq) {
        TreeMap<String, Double> wordFreq = new TreeMap<String, Double>();
        freq.forEach((k, v) -> {
            //ä¿è¯äº†å•å­—çš„è¯é¢‘å¤§æ¦‚ç‡æ¯”è¯ä½
            if (k.length() > 1 && v <= 100) {
                v = 100 * v;
            } else if (k.length() == 1 && v >= 10000) {
                v = v / 1000;
            }
            wordFreq.put(k, Math.abs(this.logTotal - Math.log(v)));
        });
        extFreq.forEach((k, v) -> {
            wordFreq.put(k, Math.abs(this.logTotal - Math.log(v)));
        });
        this.doubleArrayTrie.build(wordFreq);
    }

    /**
     * å°†æ¯ä¸€ä¸ªå­—è½¬åŒ–ä¸ºç»“ç‚¹ï¼Œå‚¨å­˜å„ç§åˆ†è¯è¿‡ç¨‹ä¸­éœ€è¦çš„ä¸´æ—¶æ•°æ®
     *
     * @param sentence
     * @return
     */
    public static ArrayList<TokenNode> buildNodes(String sentence) {
        String cleanedSen = clean(sentence);
        Matcher matcher = URL_NUM_ALPHA_CH_NUM_REG.matcher(cleanedSen);
        // éä¸­æ–‡å­—ç¬¦çš„åœ¨åŸå­—ç¬¦ä¸²çš„åç§»é‡ï¼Œkey=start offset, value=end offset
        HashMap<Integer, Integer> alphaSpanMap = new HashMap<>();
        HashMap<Integer, String> alphaTagMap = new HashMap<>();
        while (matcher.find()) {
            alphaSpanMap.put(matcher.start(), matcher.end());
            if (matcher.start(TokenNode.URL_TAG) != -1) {
                alphaTagMap.put(matcher.start(), TokenNode.URL_TAG);
            } else if (matcher.start(TokenNode.NUM_TAG) != -1) {
                alphaTagMap.put(matcher.start(), TokenNode.NUM_TAG);
            } else if (matcher.start(TokenNode.ALPHA_TAG) != -1) {
                alphaTagMap.put(matcher.start(), TokenNode.ALPHA_TAG);
            }
        }
        // å¤„ç†emojiè¡¨æƒ…ï¼Œemojiè¡¨æƒ…çš„é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„ï¼Œæ¯”å¦‚ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦=8ï¼ŒğŸ˜‚=2
        List<CustomEmojiParser.CustomEmoji> emojis = CustomEmojiParser.getUnicodeCandidates(sentence);
        int start = 0, wordNum = 0, emojiIndex = 0;
        int senLength = sentence.length();
        ArrayList<TokenNode> nodes = new ArrayList<>();
        while (start < senLength) {
            TokenNode node;
            Integer alphaSpanEnd = alphaSpanMap.get(start);
            if (alphaSpanEnd != null) {
                // æ˜¯ç‰¹æ®Šç¬¦å·
                node = new TokenNode(sentence.substring(start, alphaSpanEnd), wordNum, alphaTagMap.get(start));
                node.setCleanedText(cleanedSen.substring(start, alphaSpanEnd));
                node.setStartOffset(start);
                start = alphaSpanEnd;
            } else {
                String subCleanStr;
                String tag = null;
                if (emojis.size() > emojiIndex && emojis.get(emojiIndex).getStartOffset() == start) {
                    // æ˜¯emoji
                    subCleanStr = emojis.get(emojiIndex).getEmojiStr();
                    // emojiå½“åšæ ‡ç‚¹ç¬¦å·å¤„ç†
                    tag = TokenNode.PUNC_TAG;
                    emojiIndex++;
                } else {
                    // ç‰¹æ®Šçš„æ±‰å­—é•¿åº¦ä¸ä¸€å®šæ˜¯1ï¼Œæ¯”å¦‚ ğ¡ƒ=2
                    subCleanStr = new String(Character.toChars(cleanedSen.codePointAt(start)));
                    if (PUNC.contains(subCleanStr)) {
                        // æ ‡ç‚¹ç¬¦å·
                        tag = TokenNode.PUNC_TAG;
                    }
                }
                node = new TokenNode(sentence.substring(start, start + subCleanStr.length()), wordNum, tag);
                node.setCleanedText(subCleanStr);
                node.setStartOffset(start);
                start = start + subCleanStr.length();
            }
            nodes.add(node);

            wordNum++;
        }
        return nodes;
    }

    private static String clean(String sentence) {
        sentence = sentence.replaceAll(TextUtility.WHITESPACE_CHARCLASS, " ");
        sentence = TextUtility.sbc2dbcCase(sentence);
        String sentenceFinal = sentence;
        String simpleCN = AccessController.doPrivileged((PrivilegedAction<String>) () -> {
            return HanLP.convertToSimplifiedChinese(sentenceFinal);
        });
        // å¦‚æœopenccè¯†åˆ«æˆåŠŸ,é•¿åº¦åº”è¯¥æ˜¯ç›¸ç­‰çš„
        //openccä¼šæŠŠä¸è®¤è¯†çš„ç¹ä½“å­—è½¬æ¢æˆä¸‹é¢ğ ¡ å­—ç¬¦,è€Œè¿™ä¸ªå­—ç¬¦çš„é•¿åº¦æ˜¯2
        if (simpleCN.length() == sentence.length()) {
            sentence = simpleCN;
        }
        return TextUtility.toLowerCase(sentence);
    }

    public void reLoadMainDict() throws Exception {
        logger.info("start to reload dict.");
        // æ–°å¼€ä¸€ä¸ªå®ä¾‹åŠ è½½è¯å…¸ï¼Œå‡å°‘åŠ è½½è¿‡ç¨‹å¯¹å½“å‰è¯å…¸ä½¿ç”¨çš„å½±å“
        Dictionary tmpDict = new Dictionary(configuration);
        tmpDict.loadDict();
        instance.total = tmpDict.total;
        instance.logTotal = tmpDict.logTotal;
        instance.doubleArrayTrie = tmpDict.doubleArrayTrie;
        instance.metaWords = tmpDict.metaWords;
        logger.info("reload dict finished.");
    }

    public HashSet<String> getMetaWords() {
        return metaWords;
    }

    public double getLogTotal() {
        return logTotal;
    }

    public AhoCorasickDoubleArrayTrie getDoubleArrayTrie() {
        return doubleArrayTrie;
    }
}